{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS - Machine Learning At Scale\n",
    "## Assignment - HW4\n",
    "Data Analytics and Machine Learning at Scale\n",
    "Target, Minneapolis\n",
    "\n",
    "---\n",
    "__Name:__  Tom Hughes   \n",
    "__Class:__ DAMLAS (Section Summer 2016)     \n",
    "__Email:__  Thomas.Hughes@Target.com     \n",
    "__Week:__   04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW 4 Problems](#3)   \n",
    "    4.0.  [Final Project description](#4.0)   \n",
    "    4.1.  [Build a decision to predict whether you can play tennis or no](#4.1)   \n",
    "    4.2.  [Regression Tree (OPTIONAL Homework)](#4.2)    \n",
    "    4.3.  [Predict survival on the Titanic](#4.3)    \n",
    "    4.4.  [Heritage Healthcare Prize (Predict # Days in Hospital next year)](#4.4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "* Homework submissions are due by Thursday, 08/18/2016 at 11AM (CT).\n",
    "\n",
    "\n",
    "* Prepare a single Jupyter notebook (not a requirment), please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + [Submission Link - Google Form](http://goo.gl/forms/er3OFr5eCMWDngB72)\n",
    "\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "\n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* [Lecture Slides on Decision Trees and Ensembles](https://www.dropbox.com/s/lm4vuocqoq6mq7k/Lecture-13-Decision-Trees-PLanet.pdf?dl=0)\n",
    "\n",
    "* Chapter 17 on decision Trees,   https://www.dropbox.com/s/5ca98ah5chqlcmn/Data_Science_from_Scratch%20%281%29.pdf?dl=0   [Please do not share this PDF]\n",
    "* Karau, Holden, Konwinski, Andy, Wendell, Patrick, & Zaharia, Matei. (2015). Learning Spark: Lightning-fast big data analysis. Sebastopol, CA: O’Reilly Publishers.\n",
    "* Hastie, Trevor, Tibshirani, Robert, & Friedman, Jerome. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). Stanford, CA: Springer Science+Business Media. __(Download for free [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf))__\n",
    "* Ryza, Sandy, Laserson, Uri, Owen, Sean, & Wills, Josh. (2015). Advanced analytics with Spark: Patterns for learning from data at scale. Sebastopol, CA: O’Reilly Publishers.\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW4  <a name=\"4\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4.0\"></a>\n",
    "## HW4.0 Final Project description\n",
    "\n",
    "Please prepare your project description using the following format\n",
    "* 200 words abstract\n",
    "* data source and description\n",
    "* pipeline of steps (in a block diagram)\n",
    "* Metrics for success\n",
    "\n",
    "PLEASE NOTE: We will probably have project team sizes of 3 people plus/minus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "\n",
    "#### Team:  Mark VonOven, Earl Sun, Thomas Hughes\n",
    "\n",
    "__Situation:__   We have a record of what guests browse online.  There is no such analog for our Stores.  We know what guests buy, but do not know how they shopped.  There is value in understanding the path guests take, what material can be presented along the way and where to efficiently place our team.\n",
    "\n",
    "__Task:__  LED lights are being installed in stores which are capable of tracking guests with the Target app.  Using position information we can plot the guests' path, items they viewed and where they spent the most time.  This will allow us to understand and predict how a guest's path impacts purchases.\n",
    "\n",
    "__Data Source:__  Position data is stored within Target's hadoop environment.  The information contains x/y coordinates, yaw and a timestamp.  We will also need data on Store layout and purchases made.\n",
    "\n",
    "__Pipeline of Steps:__\n",
    "1.  Obtain position data, Store layout and purchase record\n",
    "2.  Analyze path, speed, stops and purchases for TMs and guests\n",
    "3.  Approximate location when accuracy switches from LED to Bluetooth\n",
    "4.  Predict guest path based on prior history and optimize tailored marketing\n",
    "\n",
    "__Success Metrics:__  \n",
    "1. Identify how Store path translates to sales\n",
    "2. Determine whether team member placement impacts sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"4.1\"></a>\n",
    "## HW4.1 Build a decision to predict whether you can play tennis or not\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Write a program in Python (or in Spark; this part is optional) to implement the ID3 decision tree algorithm. You should build a tree to predict PlayTennis, based on the other attributes (but, do not use the Day attribute in your tree.). You should read in a space delimited dataset in a file called dataset.txt and output to the screen your decision tree and the training set accuracy in some readable format. For example, here is the tennis dataset. The first line will contain the names of the fields:\n",
    "\n",
    "<PRE>\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no\n",
    "</PRE>\n",
    "\n",
    "The last column is the classification attribute, and will always contain contain the values yes or no.\n",
    "\n",
    "For output, you can choose how to draw the tree so long as it is clear what the tree is. You might find it easier if you turn the decision tree on its side, and use indentation to show levels of the tree as it grows from the left. For example:\n",
    "\n",
    "<PRE>\n",
    "outlook = sunny\n",
    "|  humidity = high: no\n",
    "|  humidity = normal: yes\n",
    "outlook = overcast: yes\n",
    "outlook = rainy\n",
    "|  windy = TRUE: no\n",
    "|  windy = FALSE: yes\n",
    "\n",
    "</PRE>\n",
    "\n",
    "You don't need to make your tree output look exactly like above: feel free to print out something similarly readable if you think it is easier to code.\n",
    "\n",
    "You may find Python dictionaries especially useful here, as they will give you a quick an easy way to help manage counting the number of times you see a particular attribute.\n",
    "\n",
    "Here are some FAQs that I've gotten in the past regarding this assignment, and some I might get if I don't answer them now.\n",
    "\n",
    "__Should my code work for other datasets besides the tennis dataset?__ \n",
    "Yes. We will give your program a different dataset to try it out with. You may assume that our dataset is correct and well-formatted, but you should not make assumptions regrading number of rows, number of columns, or values that will appear within. The last column will also be the classification, and will always contain yes or no values.\n",
    "\n",
    "__Is it possible that some value, like \"normal,\" could appear in more than one column?__\n",
    "Yes. In addition to the column \"humidity\", we might have had another column called \"skycolor\" which could have values \"normal,\" \"weird,\" and \"bizarre.\"\n",
    "\n",
    "__Could \"yes\" and \"no\" appear as possible values in columns other than the classification column?__\n",
    "Yes. In addition to the classification column \"playtennis,\" we might have had another column called \"seasonalweather\" which would contain \"yes\" and \"no.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tennisData.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile tennisData.txt\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2)\n",
    "        for p in class_probabilities\n",
    "        if p) # ignore zero probabilities\n",
    "        \n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)*1.0\n",
    "    return [count / total_count\n",
    "        for count in Counter(labels).values()]  \n",
    "\n",
    "    \n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)\n",
    "\n",
    "\n",
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\n",
    "    subsets is a list of lists of labeled data\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count\n",
    "                for subset in subsets )\n",
    "                \n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"each input is a pair (attribute_dict, label).\n",
    "    returns a dict : attribute_value -> inputs\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = input[0][attribute] # get the value of the specified attribute\n",
    "        groups[key].append(input) # then add this input to the correct list\n",
    "    return groups\n",
    "    \n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values()) \n",
    "        \n",
    "\n",
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "    # if this is our first pass,\n",
    "    # all keys of the first input are split candidates\n",
    "\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    if num_trues == 0: return False # no Trues? return a \"False\" leaf\n",
    "    if num_falses == 0: return True # no Falses? return a \"True\" leaf\n",
    "    \n",
    "    if not split_candidates: # if no split candidates left\n",
    "        return num_trues >= num_falses # return the majority leaf\n",
    "    \n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "                             key=partial(partition_entropy_by, inputs))\n",
    "    \n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates\n",
    "                    if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute_value : build_tree_id3(subset, new_candidates)\n",
    "                for attribute_value, subset in partitions.iteritems() }\n",
    "    \n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "    \n",
    "    return (best_attribute, subtrees)  \n",
    "    \n",
    "    \n",
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "    \n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "    \n",
    "    # otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values of are subtrees to consider next\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute) # None if input is missing attribute\n",
    "    \n",
    "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
    "        subtree_key = None # we'll use the None subtree\n",
    "    \n",
    "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
    "    return classify(subtree, input) # and use it to classify the input    \n",
    "    \n",
    "\n",
    "\n",
    "def print_tree(tree):\n",
    "    print tree[0]\n",
    "    for key in tree[1].keys():\n",
    "        if key != None:\n",
    "            if tree[1][key] == True:\n",
    "                print \"\\n\"+\" | \"+str(key)\n",
    "                print \"    \"+str(tree[1][key])\n",
    "            elif tree[1][key] == False:\n",
    "                print \"\\n\"+\" | \"+str(key)\n",
    "                print \"    \"+str(tree[1][key])\n",
    "            else:\n",
    "                print \"\\n\"+\" | \"+str(key)\n",
    "                for k in tree[1][key][1].keys():\n",
    "                    if k != None:\n",
    "                            print \"   |\" + str(tree[1][key][0])+\" = \"+str(k)\n",
    "                            print \"    \"+str(tree[1][key][1][k])\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook\n",
      "\n",
      " | rainy\n",
      "   |wind = FALSE\n",
      "    True\n",
      "   |wind = TRUE\n",
      "    False\n",
      "\n",
      " | overcast\n",
      "    True\n",
      "\n",
      " | sunny\n",
      "   |humidity = high\n",
      "    False\n",
      "   |humidity = normal\n",
      "    True\n"
     ]
    }
   ],
   "source": [
    "filenm = \"tennisData.txt\"\n",
    "\n",
    "def inputformat(data):\n",
    "    headers = []\n",
    "    input_data = []\n",
    "    with open(data) as f:\n",
    "        headers = f.readline().split(\" \") \n",
    "        headers.pop(0)\n",
    "        for line in f.readlines():\n",
    "            d = line.strip().split(\" \")\n",
    "            d.pop(0)\n",
    "            rng = len(d)-1\n",
    "            input_line = {}\n",
    "            if d[rng] == 'yes':\n",
    "                d[rng] = True\n",
    "            else:\n",
    "                d[rng] = False\n",
    "            for i in range(rng):\n",
    "                input_line[headers[i]] = d[i]\n",
    "            input_data.append((input_line, d[rng]))  \n",
    "    return input_data            \n",
    "    \n",
    "inputs2 = inputformat(filenm)\n",
    "        \n",
    "tree = build_tree_id3(inputs2)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW4.1.1 What is the classification accuracy of the tree on the training data?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def fcst_acc(data):\n",
    "    accuracy = 0\n",
    "    total = len(data)\n",
    "    for i in data:\n",
    "        if i[1] == classify(tree, i[0]):\n",
    "            accuracy += 1    \n",
    "\n",
    "    print \"accuracy is: \"+str((1.0*accuracy)/(1.0*total)*100)+\"%\"\n",
    "\n",
    "\n",
    "\n",
    "fcst_acc(inputs2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.2  Is it possible to produce some set of correct training examples that will get the algorihtm\n",
    "to include the attribute Temperature in the learned tree, even though the true target concept is\n",
    "independent of Temperature? if no, explain. If yes, give such a set. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tennisData_overfit.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile tennisData_overfit.txt\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny cool high FALSE no\n",
    "d2 sunny cool high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy hot high FALSE yes\n",
    "d5 rainy hot normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast hot normal TRUE yes\n",
    "d7 sunny cool high FALSE no\n",
    "d8 sunny hot normal FALSE yes\n",
    "d9 rainy hot normal FALSE yes\n",
    "d10 sunny hot normal TRUE yes\n",
    "d11 overcast hot high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy cool high TRUE no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The set above has the target concept output by ID3 in 4.1.  By changing the temperature values all to 'hot' when playtennis = 'yes' and 'cool' when playtennis = 'no', ID3 would split on that factor first, even though the true concept is not related to temperature.  That is the drawback of ID3.  Since it constantly searches for the optimal entropy, it can often lead to overfitting the data.  This is why random forest and boosted trees use techniques like only training on a subset of columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.3  Now, build a tree using only examples D1–D7. What is the classification accuracy for the\n",
    "training set? what is the accuracy for the test set (examples D8–D14)? explain why you think these\n",
    "are the results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tennisData_sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile tennisData_sample.txt\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook\n",
      "\n",
      " | rainy\n",
      "   |wind = FALSE\n",
      "    True\n",
      "   |wind = TRUE\n",
      "    False\n",
      "\n",
      " | overcast\n",
      "    True\n",
      "\n",
      " | sunny\n",
      "    False\n"
     ]
    }
   ],
   "source": [
    "filenm = \"tennisData_sample.txt\"\n",
    "\n",
    "def inputformat(data):\n",
    "    headers = []\n",
    "    input_data = []\n",
    "    with open(data) as f:\n",
    "        headers = f.readline().split(\" \") \n",
    "        headers.pop(0)\n",
    "        for line in f.readlines():\n",
    "            d = line.strip().split(\" \")\n",
    "            d.pop(0)\n",
    "            rng = len(d)-1\n",
    "            input_line = {}\n",
    "            if d[rng] == 'yes':\n",
    "                d[rng] = True\n",
    "            else:\n",
    "                d[rng] = False\n",
    "            for i in range(rng):\n",
    "                input_line[headers[i]] = d[i]\n",
    "            input_data.append((input_line, d[rng]))  \n",
    "    return input_data            \n",
    "    \n",
    "inputs2 = inputformat(filenm)\n",
    "        \n",
    "tree = build_tree_id3(inputs2)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tennisData_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile tennisData_test.txt\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 66.6666666667%\n"
     ]
    }
   ],
   "source": [
    "filenm = \"tennisData_test.txt\"\n",
    "\n",
    "def inputformat(data):\n",
    "    headers = []\n",
    "    input_data = []\n",
    "    with open(data) as f:\n",
    "        headers = f.readline().split(\" \") \n",
    "        headers.pop(0)\n",
    "        for line in f.readlines():\n",
    "            d = line.strip().split(\" \")\n",
    "            d.pop(0)\n",
    "            rng = len(d)-1\n",
    "            input_line = {}\n",
    "            if d[rng] == 'yes':\n",
    "                d[rng] = True\n",
    "            else:\n",
    "                d[rng] = False\n",
    "            for i in range(rng):\n",
    "                input_line[headers[i]] = d[i]\n",
    "            input_data.append((input_line, d[rng]))  \n",
    "    return input_data            \n",
    "    \n",
    "inputs2 = inputformat(filenm)\n",
    "fcst_acc(inputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy is 66.67%.  This is because the tree is overfitting to the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.4 In this case, and others, there are only a few labelled examples available for training (that\n",
    "is, no additional data is available for testing or validation). Suggest a concrete pruning strategy, that\n",
    "can be readily embedded in the algorithm, to avoid over fitting. Explain why you think this strategy\n",
    "should work.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A pruning strategy that will work for ID3 is sub-tree replacement pruning.  First, take the training data and split it randomly into a training set and a test set (you could do k-folds cross validation for this to make it more robust).  Next, grow the tree for the training set and use it to classify the test set.  After classification, remove the lowest leaves from the tree and measure the new error on the validation set.  Remove the node with the greatest error improvement.  Repeat.  This strategy works by forcing a tree from it's \"pure\" state to be more generalized and thus more effective on new/unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4.2\"></a>\n",
    " ## HW4.2 Regression Tree (OPTIONAL Homework) \n",
    " \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Implement a decision tree algorithm for regression for two input continous variables and one categorical input variable on a single core computer using Python. \n",
    "\n",
    "- Use the IRIS dataset to evaluate your code, where the input variables are: Petal.Length Petal.Width  Species  and the target or output variable is  Sepal.Length. \n",
    "- Use the same dataset to train and test your implementation. \n",
    "- Stop expanding nodes once you have less than ten (10) examples (along with the usual stopping criteria). \n",
    "- Report the mean squared error for your implementation and contrast that with the MSE from scikit-learn's implementation on this dataset (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"4.3\"></a>\n",
    "## HW4.3 Predict survival on the Titanic using Python (Logistic regression, SVMs, Random Forests)\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, you need to review (and edit the code) in this [notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/kmbgrkhh73931lo/Titanic-EDA-LogisticRegression.ipynb) to do analysis of what sorts of people were likely to survive. In particular, please look at how the tools of machine learning are used to predict which passengers survived the tragedy. Please share any usefule graphs/analysis you come up with via the group email.\n",
    "\n",
    "For more details see:\n",
    "\n",
    "* https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a link to my Google Group email:  https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!topic/target-msp-ds-camp-2016-06/-3wB2HNW-p4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4.4\"></a>\n",
    " ## HW4.4 Heritage Healthcare Prize (Predict # Days in Hospital next year)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "1. Introduction \n",
    "Back to Table of Contents\n",
    "\n",
    "The Heritage Health Prize (HHP) was a data science challenge sponsored by The Heritage Provider Network. It took place from April 4, 2011 to April 4, 2013. For information on the winning entries, please see here.\n",
    "\n",
    "Please see the following notebooks for more background and candidate solutions\n",
    "\n",
    "\n",
    "- Spark Map-Reduce + MMLlib solution (with optional extensions) See [Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/v52cxipe7yftf97/HeritageHealthPrizeUnitTestNotebook_Spark-Map-Reduce.ipynb)\n",
    "\n",
    "- Spark SQL + MLLib solution (with optional extensions): [Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/s2wxg6g982oho5m/HeritageHealthPrizeUnitTestNotebook_SQL_FINAL.ipynb)\n",
    "\n",
    "\n",
    "Please look at section 7 in both notebooks complete any one or more the suggested next steps. E.g.,\n",
    "\n",
    "* Please complete the EDA extensions using inspiration from the Titanic Notebook from above.\n",
    "* __Complete Section 3.B: EDA-0. Gather information to see what transformations may need to be done on the data.__\n",
    "Answer questions about each raw DataFrame. In general, is the data in good shape? For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers? In the Claims DataFrame (df_claims), how many different ProviderIDs are there? How many different PrimaryConditionGroups are there? What are their values? What values can the CharlesonIndex take on? Are they integers? In the Drug Count DataFrame (df_drug_count), what values can DrugCount take on? Are they all integers? Given this information, what transformations are needed?\n",
    "\n",
    "* __Complete Section 3.D: EDA-1. Create tables and graphs to display information about the transformed DataFrames. __\n",
    "For inspiration, see the Titanic notebook discussed above. Answer questions about each DataFrame. For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what is the minimum, maximum, mean, and standard deviation of DaysInHospital? In the Claims DataFrame, group by MemberID and Year and count the number of records. What is the minimum, maximum, mean, and standard deviation of the count? Do the same for the Drug Count and Lab Count DataFrames, etc.\n",
    "\n",
    "\n",
    "* __ Please generate ensemble of DT model using 100 trees with 8 nodes and report the Loss __\n",
    "Try additional models. See possibilities here (e.g. Decision Tree Regressor, Gradient-Boosted Trees Regressor, Random Forest Regressor). See an example here. Tune their hyperparameters. Try different feature selections. Try a two-step model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4_Hughes <a name=\"Hughes\"></a>\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "[Claims Analysis](#4.4Claims)\n",
    "\n",
    "[Drug Analysis](#4.4Drug)\n",
    "\n",
    "[Lab Analysis](#4.4Lab)\n",
    "\n",
    "[Member Analysis](#4.4Member)\n",
    "\n",
    "[Target_Yr2 Analysis](#4.4Y2)\n",
    "\n",
    "[Target_Yr3 Analysis](#4.4Y3)\n",
    "\n",
    "[Target_Yr4 Analysis](#4.4Y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Complete Section 3.B: EDA-0. Gather information to see what transformations may need to be done on the data.__\n",
    "Answer questions about each raw DataFrame. In general, is the data in good shape? For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers? In the Claims DataFrame (df_claims), how many different ProviderIDs are there? How many different PrimaryConditionGroups are there? What are their values? What values can the CharlesonIndex take on? Are they integers? In the Drug Count DataFrame (df_drug_count), what values can DrugCount take on? Are they all integers? Given this information, what transformations are needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x0000000003E72A20>\n",
      "<pyspark.sql.context.SQLContext object at 0x0000000008178A90>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys #current as of 7/15/2016\n",
    "\n",
    "\n",
    "spark_home = os.environ['SPARK_HOME'] = 'c:/Apps/Spark/Spark_1.6.2/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the directory containing the raw data files\n",
    "DATA_DIR = os.path.join('HealthcareData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for target_Y2 DataFrame with 76038 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|24027423|              0|             0|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for target_Y3 DataFrame with 71435 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|90963501|              0|             0|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for target_Y4 DataFrame with 70942 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|20820036|              0|           NaN|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for claims DataFrame with 2668990 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ProviderID: string (nullable = true)\n",
      " |-- Vendor: string (nullable = true)\n",
      " |-- PCP: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Specialty: string (nullable = true)\n",
      " |-- PlaceSvc: string (nullable = true)\n",
      " |-- PayDelay: string (nullable = true)\n",
      " |-- LengthOfStay: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- PrimaryConditionGroup: string (nullable = true)\n",
      " |-- CharlsonIndex: string (nullable = true)\n",
      " |-- ProcedureGroup: string (nullable = true)\n",
      " |-- SupLOS: string (nullable = true)\n",
      "\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "|MemberID|ProviderID|Vendor|  PCP|Year|Specialty|PlaceSvc|PayDelay|LengthOfStay|       DSFS|PrimaryConditionGroup|CharlsonIndex|ProcedureGroup|SupLOS|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "|42286978|   8013252|172193|37796|  Y1|  Surgery|  Office|      28|         NaN|8- 9 months|              NEUMENT|            0|           MED|     0|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for drug_count DataFrame with 818241 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- DrugCount: string (nullable = true)\n",
      "\n",
      "+--------+----+-----------+---------+\n",
      "|MemberID|Year|       DSFS|DrugCount|\n",
      "+--------+----+-----------+---------+\n",
      "|48925661|  Y2|9-10 months|       7+|\n",
      "+--------+----+-----------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for lab_count DataFrame with 361484 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- LabCount: string (nullable = true)\n",
      "\n",
      "+--------+----+-----------+--------+\n",
      "|MemberID|Year|       DSFS|LabCount|\n",
      "+--------+----+-----------+--------+\n",
      "|69258001|  Y3|2- 3 months|       1|\n",
      "+--------+----+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for members DataFrame with 113000 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- AgeAtFirstClaim: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      "\n",
      "+--------+---------------+---+\n",
      "|MemberID|AgeAtFirstClaim|Sex|\n",
      "+--------+---------------+---+\n",
      "|14723353|          70-79|  M|\n",
      "+--------+---------------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_csv_file(filename):\n",
    "    # HACK - Loading csv directly into a dataframe was supposed to work by using a Databricks\n",
    "    # package (https://github.com/databricks/spark-csv), but it didn't.\n",
    "    # So in this section, we are going the Pandas -> DataFrame route.  Could also go RDD -> DataFrame route.\n",
    "    \n",
    "    # Set our file path\n",
    "    input_path = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    # Read into Pandas dataframe.  We assume the file contains a header row.\n",
    "    # Note:  We read in all fields as string.\n",
    "    df_pandas = pd.read_csv(input_path, dtype='str')  \n",
    "    \n",
    "    # Read Pandas dataframe into SQL DataFrame.\n",
    "    # We must define the schema so that datatype is not automatically inferred and everything remains string.\n",
    "    schema_string = df_pandas.columns.values.tolist()\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schema_string]\n",
    "    schema = StructType(fields)\n",
    "    \n",
    "    return sqlContext.createDataFrame(df_pandas, schema)\n",
    "\n",
    "# Read in Year 2 Target Variables\n",
    "df_target_Y2 = load_csv_file('DaysInHospital_Y2.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y2 DataFrame with %d rows:\" %(df_target_Y2.count())\n",
    "df_target_Y2.printSchema()\n",
    "df_target_Y2.show(1)\n",
    "\n",
    "# Read in Year 3 Target Variables\n",
    "df_target_Y3 = load_csv_file('DaysInHospital_Y3.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y3 DataFrame with %d rows:\" %(df_target_Y3.count())\n",
    "df_target_Y3.printSchema()\n",
    "df_target_Y3.show(1)\n",
    "\n",
    "# Read in Year 4 Target Variables\n",
    "df_target_Y4 = load_csv_file('Target.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y4 DataFrame with %d rows:\" %(df_target_Y4.count())\n",
    "df_target_Y4.printSchema()\n",
    "df_target_Y4.show(1)\n",
    "\n",
    "# Read in Claims Data\n",
    "df_claims = load_csv_file('Claims.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for claims DataFrame with %d rows:\" %(df_claims.count())\n",
    "df_claims.printSchema()\n",
    "df_claims.show(1)\n",
    "\n",
    "# Read in Drug Data\n",
    "df_drug_count = load_csv_file('DrugCount.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for drug_count DataFrame with %d rows:\" %(df_drug_count.count())\n",
    "df_drug_count.printSchema()\n",
    "df_drug_count.show(1)\n",
    "\n",
    "# Read in Lab Data\n",
    "df_lab_count = load_csv_file('LabCount.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for lab_count DataFrame with %d rows:\" %(df_lab_count.count())\n",
    "df_lab_count.printSchema()\n",
    "df_lab_count.show(1)\n",
    "\n",
    "# Read in Members Data\n",
    "df_members = load_csv_file('Members.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for members DataFrame with %d rows:\" %(df_members.count())\n",
    "df_members.printSchema()\n",
    "df_members.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x000000000816AF60>\n"
     ]
    }
   ],
   "source": [
    "print sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv_file(filename):\n",
    "    # HACK - Loading csv directly into a dataframe was supposed to work by using a Databricks\n",
    "    # package (https://github.com/databricks/spark-csv), but it didn't.\n",
    "    # So in this section, we are going the Pandas -> DataFrame route.  Could also go RDD -> DataFrame route.\n",
    "    \n",
    "    # Set our file path\n",
    "    input_path = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    # Read into Pandas dataframe.  We assume the file contains a header row.\n",
    "    # Note:  We read in all fields as string.\n",
    "    df_pandas = pd.read_csv(input_path, dtype='str')  \n",
    "    \n",
    "    # Read Pandas dataframe into SQL DataFrame.\n",
    "    # We must define the schema so that datatype is not automatically inferred and everything remains string.\n",
    "    schema_string = df_pandas.columns.values.tolist()\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schema_string]\n",
    "    schema = StructType(fields)\n",
    "    \n",
    "    return sqlContext.createDataFrame(df_pandas, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims Data <a name=\"4.4Claims\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In the Claims DataFrame (df_claims), how many different ProviderIDs are there? How many different PrimaryConditionGroups are there? What are their values? What values can the CharlesonIndex take on? Are they integers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for claims DataFrame with 2668990 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ProviderID: string (nullable = true)\n",
      " |-- Vendor: string (nullable = true)\n",
      " |-- PCP: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Specialty: string (nullable = true)\n",
      " |-- PlaceSvc: string (nullable = true)\n",
      " |-- PayDelay: string (nullable = true)\n",
      " |-- LengthOfStay: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- PrimaryConditionGroup: string (nullable = true)\n",
      " |-- CharlsonIndex: string (nullable = true)\n",
      " |-- ProcedureGroup: string (nullable = true)\n",
      " |-- SupLOS: string (nullable = true)\n",
      "\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "|MemberID|ProviderID|Vendor|  PCP|Year|Specialty|PlaceSvc|PayDelay|LengthOfStay|       DSFS|PrimaryConditionGroup|CharlsonIndex|ProcedureGroup|SupLOS|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "|42286978|   8013252|172193|37796|  Y1|  Surgery|  Office|      28|         NaN|8- 9 months|              NEUMENT|            0|           MED|     0|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Claims Data\n",
    "df_claims = load_csv_file('Claims.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for claims DataFrame with %d rows:\" %(df_claims.count())\n",
    "df_claims.printSchema()\n",
    "df_claims.show(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MemberID', 'ProviderID', 'Vendor', 'PCP', 'Year', 'Specialty', 'PlaceSvc', 'PayDelay', 'LengthOfStay', 'DSFS', 'PrimaryConditionGroup', 'CharlsonIndex', 'ProcedureGroup', 'SupLOS']\n",
      "+----+------+\n",
      "|Year| count|\n",
      "+----+------+\n",
      "|  Y1|865689|\n",
      "|  Y2|898872|\n",
      "|  Y3|904429|\n",
      "+----+------+\n",
      "\n",
      "+--------------------+------+\n",
      "|           Specialty| count|\n",
      "+--------------------+------+\n",
      "|                 NaN|  8405|\n",
      "|Obstetrics and Gy...| 36594|\n",
      "|               Other| 92687|\n",
      "|  Diagnostic Imaging|207297|\n",
      "|          Pediatrics| 84862|\n",
      "|      Rehabilitation| 57554|\n",
      "|            Internal|672059|\n",
      "|           Emergency|126130|\n",
      "|             Surgery|208217|\n",
      "|          Laboratory|653188|\n",
      "|      Anesthesiology| 33435|\n",
      "|    General Practice|473655|\n",
      "|           Pathology| 14907|\n",
      "+--------------------+------+\n",
      "\n",
      "+-------------------+-------+\n",
      "|           PlaceSvc|  count|\n",
      "+-------------------+-------+\n",
      "|                NaN|   7632|\n",
      "|        Urgent Care| 199528|\n",
      "|              Other|  11700|\n",
      "|    Independent Lab| 657750|\n",
      "|          Ambulance|  34766|\n",
      "|Outpatient Hospital| 121528|\n",
      "| Inpatient Hospital|  85776|\n",
      "|             Office|1542007|\n",
      "|               Home|   8303|\n",
      "+-------------------+-------+\n",
      "\n",
      "+------------+-------+\n",
      "|LengthOfStay|  count|\n",
      "+------------+-------+\n",
      "|         NaN|2597392|\n",
      "|      5 days|    510|\n",
      "|  1- 2 weeks|   1143|\n",
      "|      4 days|   1473|\n",
      "|  2- 4 weeks|    961|\n",
      "|  4- 8 weeks|    903|\n",
      "|      3 days|   3246|\n",
      "|   26+ weeks|      2|\n",
      "|      2 days|   6485|\n",
      "|      6 days|    179|\n",
      "|       1 day|  56696|\n",
      "+------------+-------+\n",
      "\n",
      "+------------+------+\n",
      "|        DSFS| count|\n",
      "+------------+------+\n",
      "|         NaN| 52770|\n",
      "| 8- 9 months|171878|\n",
      "|10-11 months|116328|\n",
      "| 7- 8 months|175191|\n",
      "| 6- 7 months|180662|\n",
      "|  0- 1 month|707721|\n",
      "| 5- 6 months|192000|\n",
      "| 4- 5 months|189001|\n",
      "| 9-10 months|151527|\n",
      "| 3- 4 months|212214|\n",
      "| 2- 3 months|225216|\n",
      "| 1- 2 months|247343|\n",
      "|11-12 months| 47139|\n",
      "+------------+------+\n",
      "\n",
      "+---------------------+------+\n",
      "|PrimaryConditionGroup| count|\n",
      "+---------------------+------+\n",
      "|                  NaN| 11410|\n",
      "|              NEUMENT|171605|\n",
      "|               RESPR4|138062|\n",
      "|               INFEC4| 83552|\n",
      "|               SEPSIS|   497|\n",
      "|               STROKE|  8416|\n",
      "|              PERVALV|  3518|\n",
      "|               CANCRA|  5587|\n",
      "|               CANCRB| 42895|\n",
      "|              LIVERDZ|  2747|\n",
      "|              SEIZURE| 20501|\n",
      "|               CANCRM|  1096|\n",
      "|               PRGNCY| 32004|\n",
      "|               SKNAUT|107976|\n",
      "|               HEART2| 54207|\n",
      "|               HEART4| 28733|\n",
      "|                  CHF| 13316|\n",
      "|               RENAL1|   602|\n",
      "|               RENAL2| 10922|\n",
      "|               RENAL3| 52214|\n",
      "+---------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-------+\n",
      "|CharlsonIndex|  count|\n",
      "+-------------+-------+\n",
      "|           5+|   5989|\n",
      "|          3-4|  49479|\n",
      "|            0|1356995|\n",
      "|          1-2|1256527|\n",
      "+-------------+-------+\n",
      "\n",
      "+--------------+-------+\n",
      "|ProcedureGroup|  count|\n",
      "+--------------+-------+\n",
      "|           NaN|   3675|\n",
      "|           SGS|   9406|\n",
      "|           SAS|   5745|\n",
      "|           SNS|   7796|\n",
      "|           SUS|   5612|\n",
      "|           RAD| 265272|\n",
      "|           SIS|  56461|\n",
      "|           SCS| 274805|\n",
      "|          ANES|  17061|\n",
      "|          SMCD|   3376|\n",
      "|            PL| 492919|\n",
      "|           SDS|  60678|\n",
      "|            EM|1048210|\n",
      "|           SRS|   7905|\n",
      "|           MED| 372101|\n",
      "|          SEOA|   8420|\n",
      "|            SO|    371|\n",
      "|           SMS|  29177|\n",
      "+--------------+-------+\n",
      "\n",
      "+------+-------+\n",
      "|SupLOS|  count|\n",
      "+------+-------+\n",
      "|     0|2657658|\n",
      "|     1|  11332|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Direct queries of DataFrames\n",
    "# DataFrame Operations:  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "# Example: Get counts of values in each column of Claims data\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "print df_claims.columns\n",
    "columns_for_analysis = ['Year', 'Specialty', 'PlaceSvc', 'LengthOfStay', 'DSFS', \n",
    "                        'PrimaryConditionGroup', 'CharlsonIndex', 'ProcedureGroup', 'SupLOS']\n",
    "\n",
    "\n",
    "\n",
    "for column in columns_for_analysis:\n",
    "    df_claims.groupBy(column).count().show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'),\n",
       " ('ProviderID', 'string'),\n",
       " ('Vendor', 'string'),\n",
       " ('PCP', 'string'),\n",
       " ('Year', 'string'),\n",
       " ('Specialty', 'string'),\n",
       " ('PlaceSvc', 'string'),\n",
       " ('PayDelay', 'string'),\n",
       " ('LengthOfStay', 'string'),\n",
       " ('DSFS', 'string'),\n",
       " ('PrimaryConditionGroup', 'string'),\n",
       " ('CharlsonIndex', 'string'),\n",
       " ('ProcedureGroup', 'string'),\n",
       " ('SupLOS', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claims = load_csv_file('Claims.csv').cache()\n",
    "df_claims.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|ProviderID|count|\n",
      "+----------+-----+\n",
      "|    944326|  208|\n",
      "|   4028868| 1953|\n",
      "|   2191832| 1703|\n",
      "|    455068|   13|\n",
      "|   7202236|   17|\n",
      "|   2147967|  314|\n",
      "|   4112611|  212|\n",
      "|   6168173|   41|\n",
      "|   1192364|   45|\n",
      "|   3584196|  107|\n",
      "|   8840994|   31|\n",
      "|   8843667|   40|\n",
      "|   9376562|    9|\n",
      "|   3712896|    9|\n",
      "|   9388307|    4|\n",
      "|    671509|    3|\n",
      "|   4910521|    4|\n",
      "|   4547282|    6|\n",
      "|   2864312|   14|\n",
      "|    892252|   21|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_claims.groupby('ProviderID').count().distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(ProviderID)|\n",
      "+-----------------+\n",
      "|            14700|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_claims.agg(func.countDistinct('ProviderID')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(PrimaryConditionGroup)|\n",
      "+----------------------------+\n",
      "|                          46|\n",
      "+----------------------------+\n",
      "\n",
      "+---------------------+------+\n",
      "|PrimaryConditionGroup| count|\n",
      "+---------------------+------+\n",
      "|                  NaN| 11410|\n",
      "|              NEUMENT|171605|\n",
      "|               RESPR4|138062|\n",
      "|               INFEC4| 83552|\n",
      "|               SEPSIS|   497|\n",
      "|               STROKE|  8416|\n",
      "|              PERVALV|  3518|\n",
      "|               CANCRA|  5587|\n",
      "|               CANCRB| 42895|\n",
      "|              LIVERDZ|  2747|\n",
      "|              SEIZURE| 20501|\n",
      "|               CANCRM|  1096|\n",
      "|               PRGNCY| 32004|\n",
      "|               SKNAUT|107976|\n",
      "|               HEART2| 54207|\n",
      "|               HEART4| 28733|\n",
      "|                  CHF| 13316|\n",
      "|               RENAL1|   602|\n",
      "|               RENAL2| 10922|\n",
      "|               RENAL3| 52214|\n",
      "|                ROAMI| 48821|\n",
      "|               MISCL1|  4892|\n",
      "|               MISCL5| 48307|\n",
      "|             ARTHSPIN|288285|\n",
      "|               PNCRDZ|   912|\n",
      "|              PERINTL|   980|\n",
      "|                  AMI| 34805|\n",
      "|              APPCHOL| 17945|\n",
      "|               CATAST|  2070|\n",
      "|               TRAUMA| 72050|\n",
      "|                HIPFX|  4355|\n",
      "|                PNEUM| 11333|\n",
      "|                  UTI| 44806|\n",
      "|               MSC2a3|507277|\n",
      "|               HEMTOL| 31631|\n",
      "|               METAB1|  3863|\n",
      "|               METAB3|320553|\n",
      "|                 COPD| 44154|\n",
      "|             GIOBSENT|  9718|\n",
      "|              GIBLEED|101846|\n",
      "|              FLaELEC|  5263|\n",
      "|               GYNEC1| 44143|\n",
      "|              FXDISLC| 40851|\n",
      "|              ODaBNCA| 46732|\n",
      "|              MISCHRT|131047|\n",
      "|               GYNECA| 12491|\n",
      "+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_claims.agg(func.countDistinct('PrimaryConditionGroup')).show()\n",
    "df_claims.groupby('PrimaryConditionGroup').count().distinct().show(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(CharlsonIndex)|\n",
      "+--------------------+\n",
      "|                   4|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+-------+\n",
      "|CharlsonIndex|  count|\n",
      "+-------------+-------+\n",
      "|           5+|   5989|\n",
      "|          3-4|  49479|\n",
      "|            0|1356995|\n",
      "|          1-2|1256527|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_claims.agg(func.countDistinct('CharlsonIndex')).show()\n",
    "df_claims.groupby('CharlsonIndex').count().distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the values in the claims table are strings.  There are large numbers of null values across the claims dataframe.  In order to clean this up for use one would need to impute data for nulls (or remove altogether), recast any strings as integers, like length of stay or pay delay and update categorical field data to integerst (0, 1-2 becomes 1, 5+ becomes 5, etc).  Eventually, one will also one-hot encode the variables to transform categorical variables into boolean columns (i.e Gender becomes a column for Male and a column for Female with possible values of 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug Data <a name=\"4.4Drug\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In the Drug Count DataFrame (df_drug_count), what values can DrugCount take on? Are they all integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for drug_count DataFrame with 818241 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- DrugCount: string (nullable = true)\n",
      "\n",
      "+--------+----+-----------+---------+\n",
      "|MemberID|Year|       DSFS|DrugCount|\n",
      "+--------+----+-----------+---------+\n",
      "|48925661|  Y2|9-10 months|       7+|\n",
      "+--------+----+-----------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Drug Data\n",
    "df_drug_count = load_csv_file('DrugCount.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for drug_count DataFrame with %d rows:\" %(df_drug_count.count())\n",
    "df_drug_count.printSchema()\n",
    "df_drug_count.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'),\n",
       " ('Year', 'string'),\n",
       " ('DSFS', 'string'),\n",
       " ('DrugCount', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drug_count.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|DrugCount| count|\n",
      "+---------+------+\n",
      "|        1|263501|\n",
      "|        2|188559|\n",
      "|        3|129881|\n",
      "|        4| 87783|\n",
      "|        5| 57768|\n",
      "|        6| 36731|\n",
      "|       7+| 54018|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_drug_count.groupby('DrugCount').count().distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|MemberID|count|\n",
      "+--------+-----+\n",
      "|27379248|   21|\n",
      "|67598133|   24|\n",
      "|66711434|   32|\n",
      "|36244478|   33|\n",
      "|83078422|   18|\n",
      "|27422357|    4|\n",
      "|52820571|   18|\n",
      "| 7988901|   35|\n",
      "|65286285|   28|\n",
      "|37508582|   23|\n",
      "| 8760515|   23|\n",
      "|85513602|   27|\n",
      "|81777293|    7|\n",
      "|70582587|   35|\n",
      "|82986308|   13|\n",
      "|14877553|   18|\n",
      "| 7693430|   29|\n",
      "|36315307|   18|\n",
      "|69538151|   20|\n",
      "|39501460|   31|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+------+\n",
      "|Year| count|\n",
      "+----+------+\n",
      "|  Y1|281619|\n",
      "|  Y2|276027|\n",
      "|  Y3|260595|\n",
      "+----+------+\n",
      "\n",
      "+------------+------+\n",
      "|        DSFS| count|\n",
      "+------------+------+\n",
      "| 8- 9 months| 62700|\n",
      "|10-11 months| 46184|\n",
      "| 7- 8 months| 67433|\n",
      "| 6- 7 months| 69350|\n",
      "|  0- 1 month|108980|\n",
      "| 5- 6 months| 72274|\n",
      "| 4- 5 months| 73325|\n",
      "| 9-10 months| 55984|\n",
      "| 3- 4 months| 77044|\n",
      "| 2- 3 months| 78528|\n",
      "| 1- 2 months| 81395|\n",
      "|11-12 months| 25044|\n",
      "+------------+------+\n",
      "\n",
      "+---------+------+\n",
      "|DrugCount| count|\n",
      "+---------+------+\n",
      "|        1|263501|\n",
      "|        2|188559|\n",
      "|        3|129881|\n",
      "|        4| 87783|\n",
      "|        5| 57768|\n",
      "|        6| 36731|\n",
      "|       7+| 54018|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_for_analysis = ['MemberID', 'Year', 'DSFS', 'DrugCount']\n",
    "for column in columns_for_analysis:\n",
    "    df_drug_count.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            MemberID|\n",
      "+-------+--------------------+\n",
      "|  count|              818241|\n",
      "|   mean| 4.996952043978241E7|\n",
      "| stddev|2.8939260548182726E7|\n",
      "|    min|            10000665|\n",
      "|    max|            99998627|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_drug_count.describe(['MemberID']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Drug Count dataframe appears to be in much better shape than the claims dataframe.  There don't appear to be null entries.  Columns are structured as strings and would need to be recast, if needed.  One would also need to combine duplicate member IDs by Year while keeping track of the number of original records.  This can be done with sums, averages, min, max, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Data <a name=\"4.4Lab\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In the Lab Count DataFrame (df_lab_count), what values can LabCount take on? Are they all integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for lab_count DataFrame with 361484 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- LabCount: string (nullable = true)\n",
      "\n",
      "+--------+----+-----------+--------+\n",
      "|MemberID|Year|       DSFS|LabCount|\n",
      "+--------+----+-----------+--------+\n",
      "|69258001|  Y3|2- 3 months|       1|\n",
      "+--------+----+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Lab Data\n",
    "df_lab_count = load_csv_file('LabCount.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for lab_count DataFrame with %d rows:\" %(df_lab_count.count())\n",
    "df_lab_count.printSchema()\n",
    "df_lab_count.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'),\n",
       " ('Year', 'string'),\n",
       " ('DSFS', 'string'),\n",
       " ('LabCount', 'string')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lab_count.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|MemberID|count|\n",
      "+--------+-----+\n",
      "| 9606836|    8|\n",
      "|38282005|    9|\n",
      "|55006914|    6|\n",
      "|14991150|    8|\n",
      "|50666100|    9|\n",
      "| 2249261|   36|\n",
      "| 7375082|    4|\n",
      "|60552053|    6|\n",
      "|17279194|   32|\n",
      "| 2175588|    6|\n",
      "|40216870|    4|\n",
      "|  257428|    4|\n",
      "|16891988|    3|\n",
      "|24563556|    6|\n",
      "| 6246086|    2|\n",
      "| 3441743|    4|\n",
      "|95385376|    8|\n",
      "|88621960|    2|\n",
      "|89761766|    4|\n",
      "|27422357|    3|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+------+\n",
      "|Year| count|\n",
      "+----+------+\n",
      "|  Y1|120162|\n",
      "|  Y2|122416|\n",
      "|  Y3|118906|\n",
      "+----+------+\n",
      "\n",
      "+------------+-----+\n",
      "|        DSFS|count|\n",
      "+------------+-----+\n",
      "| 8- 9 months|23455|\n",
      "|10-11 months|16054|\n",
      "| 7- 8 months|24724|\n",
      "| 6- 7 months|26103|\n",
      "|  0- 1 month|97581|\n",
      "| 5- 6 months|27460|\n",
      "| 4- 5 months|26432|\n",
      "| 9-10 months|20126|\n",
      "| 3- 4 months|29750|\n",
      "| 2- 3 months|30305|\n",
      "| 1- 2 months|32279|\n",
      "|11-12 months| 7215|\n",
      "+------------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|LabCount|count|\n",
      "+--------+-----+\n",
      "|       1|93744|\n",
      "|       2|54183|\n",
      "|       3|43472|\n",
      "|       4|38815|\n",
      "|       5|34900|\n",
      "|       6|27705|\n",
      "|       7|20591|\n",
      "|       8|14885|\n",
      "|       9|10079|\n",
      "|     10+|23110|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_for_analysis = ['MemberID', 'Year', 'DSFS', 'LabCount']\n",
    "for column in columns_for_analysis:\n",
    "    df_lab_count.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            MemberID|\n",
      "+-------+--------------------+\n",
      "|  count|              361484|\n",
      "|   mean|5.0017938519298226E7|\n",
      "| stddev|2.8956121975431975E7|\n",
      "|    min|            10000665|\n",
      "|    max|            99998824|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lab_count.describe(['MemberID']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Lab Count dataframe appears to be in much better shape than the claims dataframe.  There don't appear to be null entries.  Columns are structured as strings and would need to be recast, if needed.  One would also need to combine duplicate member IDs by Year while keeping track of the number of original records.  This can be done with sums, averages, min, max, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Member Data <a name=\"4.4Member\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In the Member DataFrame, what values are present? Are they all integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for members DataFrame with 113000 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- AgeAtFirstClaim: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      "\n",
      "+--------+---------------+---+\n",
      "|MemberID|AgeAtFirstClaim|Sex|\n",
      "+--------+---------------+---+\n",
      "|14723353|          70-79|  M|\n",
      "+--------+---------------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Members Data\n",
    "df_members = load_csv_file('Members.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for members DataFrame with %d rows:\" %(df_members.count())\n",
    "df_members.printSchema()\n",
    "df_members.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'), ('AgeAtFirstClaim', 'string'), ('Sex', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_members.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|MemberID|count|\n",
      "+--------+-----+\n",
      "|17279194|    1|\n",
      "|90347313|    1|\n",
      "| 5249057|    1|\n",
      "|18032916|    1|\n",
      "|51730545|    1|\n",
      "|19787095|    1|\n",
      "|13509475|    1|\n",
      "|90062913|    1|\n",
      "|93146634|    1|\n",
      "|96110658|    1|\n",
      "|78556616|    1|\n",
      "|  788500|    1|\n",
      "|89852530|    1|\n",
      "|44486701|    1|\n",
      "| 8760515|    1|\n",
      "| 7028708|    1|\n",
      "| 9462205|    1|\n",
      "|54260751|    1|\n",
      "|15761488|    1|\n",
      "|78850646|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|AgeAtFirstClaim|count|\n",
      "+---------------+-----+\n",
      "|            NaN| 5753|\n",
      "|          10-19|11319|\n",
      "|          50-59|13329|\n",
      "|          40-49|16111|\n",
      "|            80+| 7621|\n",
      "|            0-9|10791|\n",
      "|          30-39|12435|\n",
      "|          70-79|14514|\n",
      "|          20-29| 8505|\n",
      "|          60-69|12622|\n",
      "+---------------+-----+\n",
      "\n",
      "+---+-----+\n",
      "|Sex|count|\n",
      "+---+-----+\n",
      "|NaN|17552|\n",
      "|  F|51482|\n",
      "|  M|43966|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_for_analysis = ['MemberID', 'AgeAtFirstClaim', 'Sex']\n",
    "for column in columns_for_analysis:\n",
    "    df_members.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Member Data is not as clean as the Drug or Lab data.  It has null values for gender and age at first claim.  I would recommend dropping the null values in this dataframe as they are not readily imputable.  One would also need to one-hot encode gender and age at first claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target_Yr2 Data <a name=\"4.4Y2\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In each of the Target DataFrames (df_target_Y2, df_target_Y3, df_target_Y4), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for target_Y2 DataFrame with 76038 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|24027423|              0|             0|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Year 2 Target Variables\n",
    "df_target_Y2 = load_csv_file('DaysInHospital_Y2.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y2 DataFrame with %d rows:\" %(df_target_Y2.count())\n",
    "df_target_Y2.printSchema()\n",
    "df_target_Y2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'),\n",
       " ('ClaimsTruncated', 'string'),\n",
       " ('DaysInHospital', 'string')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_Y2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|MemberID|count|\n",
      "+--------+-----+\n",
      "|28808907|    1|\n",
      "|27346839|    1|\n",
      "|39501460|    1|\n",
      "| 7375082|    1|\n",
      "| 8738411|    1|\n",
      "|28800833|    1|\n",
      "|79042057|    1|\n",
      "|78922204|    1|\n",
      "|66625953|    1|\n",
      "|50893522|    1|\n",
      "|73327222|    1|\n",
      "|84915601|    1|\n",
      "|22079844|    1|\n",
      "|21166770|    1|\n",
      "|66948270|    1|\n",
      "|59418162|    1|\n",
      "| 1487835|    1|\n",
      "|47684717|    1|\n",
      "|17741947|    1|\n",
      "|17646448|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|ClaimsTruncated|count|\n",
      "+---------------+-----+\n",
      "|              0|72067|\n",
      "|              1| 3971|\n",
      "+---------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|DaysInHospital|count|\n",
      "+--------------+-----+\n",
      "|             0|64269|\n",
      "|             1| 4835|\n",
      "|             2| 2366|\n",
      "|             3| 1453|\n",
      "|             4|  977|\n",
      "|             5|  565|\n",
      "|             6|  373|\n",
      "|             7|  256|\n",
      "|             8|  173|\n",
      "|             9|  148|\n",
      "|            10|  106|\n",
      "|            11|   80|\n",
      "|            12|   73|\n",
      "|            13|   61|\n",
      "|            14|   42|\n",
      "|            15|  261|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_for_analysis = ['MemberID', 'ClaimsTruncated', 'DaysInHospital']\n",
    "for column in columns_for_analysis:\n",
    "    df_target_Y2.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target_Yr3 Data <a name=\"4.4Y3\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In each of the Target DataFrames (df_target_Y2, df_target_Y3, df_target_Y4), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for target_Y3 DataFrame with 71435 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|90963501|              0|             0|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Year 3 Target Variables\n",
    "df_target_Y3 = load_csv_file('DaysInHospital_Y3.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y3 DataFrame with %d rows:\" %(df_target_Y3.count())\n",
    "df_target_Y3.printSchema()\n",
    "df_target_Y3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'),\n",
       " ('ClaimsTruncated', 'string'),\n",
       " ('DaysInHospital', 'string')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_Y3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|MemberID|count|\n",
      "+--------+-----+\n",
      "|18927481|    1|\n",
      "|30238844|    1|\n",
      "|49057069|    1|\n",
      "|43332540|    1|\n",
      "|58660696|    1|\n",
      "|15761488|    1|\n",
      "|48590284|    1|\n",
      "| 8954484|    1|\n",
      "|42558734|    1|\n",
      "| 7375082|    1|\n",
      "|89861396|    1|\n",
      "|21687169|    1|\n",
      "|81284795|    1|\n",
      "| 8159324|    1|\n",
      "|36244478|    1|\n",
      "|81952531|    1|\n",
      "|35315443|    1|\n",
      "|98711556|    1|\n",
      "|76756557|    1|\n",
      "|73644635|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|ClaimsTruncated|count|\n",
      "+---------------+-----+\n",
      "|              0|67391|\n",
      "|              1| 4044|\n",
      "+---------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|DaysInHospital|count|\n",
      "+--------------+-----+\n",
      "|             0|60706|\n",
      "|             1| 4464|\n",
      "|             2| 2182|\n",
      "|             3| 1429|\n",
      "|             4|  842|\n",
      "|             5|  528|\n",
      "|             6|  287|\n",
      "|             7|  218|\n",
      "|             8|  143|\n",
      "|             9|  115|\n",
      "|            10|  103|\n",
      "|            11|   65|\n",
      "|            12|   62|\n",
      "|            13|   50|\n",
      "|            14|   23|\n",
      "|            15|  218|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in columns_for_analysis:\n",
    "    df_target_Y3.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target_Yr4 Data <a name=\"4.4Y4\"></a>\n",
    "\n",
    "[Back to 4.4](#Hughes)\n",
    "\n",
    "In each of the Target DataFrames (df_target_Y2, df_target_Y3, df_target_Y4), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for target_Y4 DataFrame with 70942 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|20820036|              0|           NaN|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Year 4 Target Variables\n",
    "df_target_Y4 = load_csv_file('Target.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y4 DataFrame with %d rows:\" %(df_target_Y4.count())\n",
    "df_target_Y4.printSchema()\n",
    "df_target_Y4.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberID', 'string'),\n",
       " ('ClaimsTruncated', 'string'),\n",
       " ('DaysInHospital', 'string')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_Y4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|MemberID|count|\n",
      "+--------+-----+\n",
      "|56778797|    1|\n",
      "|79455022|    1|\n",
      "|28757067|    1|\n",
      "|10100129|    1|\n",
      "|54636033|    1|\n",
      "|81617254|    1|\n",
      "|47970610|    1|\n",
      "|62223236|    1|\n",
      "|22950189|    1|\n",
      "|58943638|    1|\n",
      "|14877553|    1|\n",
      "|36315307|    1|\n",
      "| 8159324|    1|\n",
      "|33683928|    1|\n",
      "|49226403|    1|\n",
      "| 7693430|    1|\n",
      "|29762321|    1|\n",
      "|40216870|    1|\n",
      "|27346839|    1|\n",
      "| 8738411|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|ClaimsTruncated|count|\n",
      "+---------------+-----+\n",
      "|              0|66974|\n",
      "|              1| 3968|\n",
      "+---------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|DaysInHospital|count|\n",
      "+--------------+-----+\n",
      "|           NaN|70942|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in columns_for_analysis:\n",
    "    df_target_Y4.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Days in Hospital for Yr2 and Yr3 are strings from 0 to 15.  These will need to be converted to integer.  The values for Days in Hospital is null for Yr4 because this is the target value.  Claims Truncated take on the value 0 or 1.  Currently they are string values.  They will need to be converted to integer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
