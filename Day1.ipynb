{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EnronNaiveBayesTrainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EnronNaiveBayesTrainer.py\n",
    "#HW 1.3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import sys, re, string, operator\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "class EnronNaiveBayesTrainer(MRJob):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(EnronNaiveBayesTrainer, self).__init__(*args, **kwargs)\n",
    "        self.modelStats = {}\n",
    "\n",
    "\n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(EnronNaiveBayesTrainer, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Don't actually yield anything for each line. Instead, collect them\n",
    "        # and yield the sums when all lines have been processed. The results\n",
    "        # will be collected by the reducer.\n",
    "        docID, docClass,text = line.split(\"\\t\",2) \n",
    "        text = text.strip()\n",
    "        text = regex.sub(' ', text.lower())\n",
    "        text = re.sub( '\\s+', ' ', text )\n",
    "        words = text.split()\n",
    "        if docID != \"D5\":  #skip doc d5 in chinese dataset\n",
    "            if docClass == \"1\":\n",
    "                yield(\"TomsPriors\", \"0,1\")\n",
    "                for word in words:\n",
    "                    yield(word, \"0,1\")\n",
    "            else:\n",
    "                yield(\"TomsPriors\", \"1,0\")\n",
    "                for word in words:\n",
    "                    yield(word, \"1,0\")\n",
    "        \n",
    "\n",
    "    def reducer(self, word, values):\n",
    "        #aggregate counts for Pr(Word|Class)\n",
    "        #yield(\"number of values for \"+word, str(values))\n",
    "        w0Total=0\n",
    "        w1Total=0\n",
    "        for value in values:\n",
    "            w0, w1 =  value.split(\",\")\n",
    "            w0Total += float(w0)\n",
    "            w1Total += float(w1)  \n",
    "        self.modelStats[word] =  [w0Total, w1Total]\n",
    "\n",
    "        #yield(\"JIMI \"+word, [w0Total, w1Total])\n",
    "    def reducer_final(self):       \n",
    "        class0Total = 0\n",
    "        class1Total = 0\n",
    "        for k in self.modelStats.keys():\n",
    "            if k != \"TomsPriors\":\n",
    "                class0Total += self.modelStats[k][0]\n",
    "                class1Total += self.modelStats[k][1]\n",
    "        vocabularySize = len(self.modelStats.keys()) -1  #ignore TomsPriors\n",
    "        #some yields to see some model internal parameters\n",
    "        #yield (\"defaultPrior 0 class\", class0Total+vocabularySize)\n",
    "        #yield (\"defaultPrior 1 class\", class1Total+vocabularySize)\n",
    "        #yield (\"count 0 class\", class0Total)\n",
    "        #yield (\"count 1 class\", class1Total)\n",
    "        #yield (\"vocabularySize\", vocabularySize)\n",
    "        \n",
    "        #calculate priors \n",
    "        classCount0, classCount1 = self.modelStats.get(\"TomsPriors\")\n",
    "        del self.modelStats[\"TomsPriors\"]\n",
    "        total = classCount0 + classCount1\n",
    "        yield(\"TomsPriors\", ','.join(str(j) for j in [classCount0, classCount1, classCount0/total, classCount1/total])) \n",
    "        for k in self.modelStats.keys():\n",
    "            yield(k, ','.join(str(j) for j in [self.modelStats[k][0],\n",
    "                      self.modelStats[k][1],\n",
    "                      self.modelStats[k][0] / class0Total,   \n",
    "                      self.modelStats[k][1] / class1Total]))                         \n",
    "                      # smoothing (self.modelStats[k][0] + 1) /(class0Total + vocabularySize), \n",
    "                      # smoothing (self.modelStats[k][1] +1)/(class1Total+vocabularySize)]))\n",
    "                            \n",
    "        #print(self.modelStats[\"assistance\"])\n",
    "        #print(vocabularySize)\n",
    "        #print(class0Total, class1Total)\n",
    "# The if __name__ == \"__main__\": \n",
    "# ... trick exists in Python so that our Python files \n",
    "# can act as either reusable modules, or as standalone programs.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    EnronNaiveBayesTrainer.run(),\n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"TomsPriors\"\t\"1.0,3.0,0.25,0.75\"\n",
      "\"beijing\"\t\"0.0,1.0,0.0,0.125\"\n",
      "\"chinese\"\t\"1.0,5.0,0.333333333333,0.625\"\n",
      "\"tokyo\"\t\"1.0,0.0,0.333333333333,0.0\"\n",
      "\"shanghai\"\t\"0.0,1.0,0.0,0.125\"\n",
      "\"japan\"\t\"1.0,0.0,0.333333333333,0.0\"\n",
      "\"macao\"\t\"0.0,1.0,0.0,0.125\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory c:\\users\\z030757\\appdata\\local\\temp\\EnronNaiveBayesTrainer.Z030757.20160712.160443.895000\n",
      "Running step 1 of 1...\n",
      "Streaming final output from c:\\users\\z030757\\appdata\\local\\temp\\EnronNaiveBayesTrainer.Z030757.20160712.160443.895000\\output...\n",
      "Removing temp directory c:\\users\\z030757\\appdata\\local\\temp\\EnronNaiveBayesTrainer.Z030757.20160712.160443.895000...\n"
     ]
    }
   ],
   "source": [
    "!python EnronNaiveBayesTrainer.py chineseExample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRNaiveBayesTrainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRNaiveBayesTrainer.py\n",
    "\n",
    "\"\"\"An implementation of a multinomial Naive Bayes learner as an MRJob.\n",
    "   This is meant as an example of why mapper_final is useful.\n",
    "   \n",
    "   This learning algorithm implementation can be further optimised. HOW?\n",
    "   \n",
    "   Use a cool pattern to do this!\n",
    "\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRNaiveBayesTrainer(MRJob):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRNaiveBayesTrainer, self).__init__(*args, **kwargs)\n",
    "        self.modelStats = {}\n",
    "        self.classTotalFreq = [0, 0]\n",
    "        self.vocab=0\n",
    "\n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MRNaiveBayesTrainer, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Don't actually yield anything for each line. Instead, collect them\n",
    "        # and yield the sums when all lines have been processed. The results\n",
    "        # will be collected by the reducer.\n",
    "        docID, docClass,text = line.split(\"\\t\",2)   \n",
    "        words = text.split()\n",
    "        vocab = {}\n",
    "        if docID != \"D5\":  #skip doc d5 in chinese dataset\n",
    "            if docClass == \"1\":\n",
    "                yield(\"TomsPriors\", \"0,1\")\n",
    "                yield(\"*classTotalFreq\", (\"0, \" + str(len(words))))\n",
    "                for word in words:\n",
    "                    vocab[word] = 1\n",
    "                    yield(word, \"0,1\")\n",
    "            else:\n",
    "                yield(\"TomsPriors\", \"1,0\")\n",
    "                yield(\"*classTotalFreq\", (str(len(words)) + \", 0\"))\n",
    "                for word in words:\n",
    "                    vocab[word] = 1\n",
    "                    yield(word, \"1,0\")\n",
    "        for k in vocab.keys():\n",
    "            yield \"*!\"+k, \"1,0\"\n",
    "        \n",
    "\n",
    "    def reducer(self, word, values):    \n",
    "        #aggregate counts for Pr(Word|Class)\n",
    "        #yield(\"number of values for \"+word, str(values))\n",
    "        w0Total=0\n",
    "        w1Total=0\n",
    "        c0Total=0\n",
    "        c1Total=1\n",
    "        for value in values:\n",
    "            w0, w1 =  value.split(\",\")\n",
    "            w0Total += float(w0)\n",
    "            w1Total += float(w1)  \n",
    "        if word == \"*classTotalFreq\":\n",
    "            self.modelStats[word] = [w0Total, w1Total]\n",
    "        elif word.startswith(\"*!\"):\n",
    "            self.vocab += 1\n",
    "        elif word == \"TomsPriors\":\n",
    "            yield(\"TomsPriors\", ','.join(str(j) for j in [w0Total,w1Total,w0Total/(w0Total+w1Total),w1Total/(w0Total+w1Total)]))\n",
    "        else:\n",
    "            yield(word, ','.join(str(j) for j in [w0Total,w1Total,(w0Total+1)/(self.modelStats[\"*classTotalFreq\"][0] + self.vocab),(w1Total+1)/(self.modelStats[\"*classTotalFreq\"][1] + self.vocab)]))\n",
    "        #yield(\"JIMI \"+word, [w0Total, w1Total])\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRNaiveBayesTrainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Beijing\"\t\"0.0,1.0,0.111111111111,0.142857142857\"\n",
      "\"Chinese\"\t\"1.0,5.0,0.222222222222,0.428571428571\"\n",
      "\"Japan\"\t\"1.0,0.0,0.222222222222,0.0714285714286\"\n",
      "\"Macao\"\t\"0.0,1.0,0.111111111111,0.142857142857\"\n",
      "\"Shanghai\"\t\"0.0,1.0,0.111111111111,0.142857142857\"\n",
      "\"Tokyo\"\t\"1.0,0.0,0.222222222222,0.0714285714286\"\n",
      "\"TomsPriors\"\t\"1.0,3.0,0.25,0.75\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory c:\\users\\z030757\\appdata\\local\\temp\\MRNaiveBayesTrainer.Z030757.20160712.183607.015000\n",
      "Running step 1 of 1...\n",
      "Streaming final output from c:\\users\\z030757\\appdata\\local\\temp\\MRNaiveBayesTrainer.Z030757.20160712.183607.015000\\output...\n",
      "Removing temp directory c:\\users\\z030757\\appdata\\local\\temp\\MRNaiveBayesTrainer.Z030757.20160712.183607.015000...\n"
     ]
    }
   ],
   "source": [
    "!python MRNaiveBayesTrainer.py chineseExample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing 0.0,1.0,0.111111111111,0.142857142857\n",
      "Chinese 1.0,5.0,0.222222222222,0.428571428571\n",
      "Japan 1.0,0.0,0.222222222222,0.0714285714286\n",
      "Macao 0.0,1.0,0.111111111111,0.142857142857\n",
      "Shanghai 0.0,1.0,0.111111111111,0.142857142857\n",
      "Tokyo 1.0,0.0,0.222222222222,0.0714285714286\n",
      "TomsPriors 1.0,3.0,0.25,0.75\n",
      "{'Beijing': '0.0,1.0,0.111111111111,0.142857142857', 'Chinese': '1.0,5.0,0.222222222222,0.428571428571', 'Tokyo': '1.0,0.0,0.222222222222,0.0714285714286', 'Shanghai': '0.0,1.0,0.111111111111,0.142857142857', 'TomsPriors': '1.0,3.0,0.25,0.75', 'Japan': '1.0,0.0,0.222222222222,0.0714285714286', 'Macao': '0.0,1.0,0.111111111111,0.142857142857'}\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "# We have two ways to run the Naive Bayes algorithm\n",
    "# 1. Run using the command line (shown Above)\n",
    "# 2. Run using a MRJob Runner from python (very sweet way to do business). See Here\n",
    "#------------------------------------------------------------------------------------\n",
    "#HW 1.3\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from MRNaiveBayesTrainer import MRNaiveBayesTrainer \n",
    "\n",
    "# STEP 1: Train a mulitnomial Naive Bayes      \n",
    "trainingData = 'chineseExample.txt'\n",
    "\n",
    "# create an instance of the Trainer class\n",
    "# and initiatialize it\n",
    "mr_job = MRNaiveBayesTrainer(args=[trainingData])\n",
    "modelStats={}\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access to the output reducer/reducer_final of \n",
    "    # the last step in MRNaiveBayesTrainer\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "        modelStats[key] = value            \n",
    "    # Store model locally\n",
    "    with open('StatelessModel1.txt', 'w') as f:\n",
    "        for k in modelStats.keys():\n",
    "            f.writelines( k + \"\\t\"+ str(modelStats[k]) +\"\\n\")\n",
    "print modelStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier for MR Naive Bayes.  Hasn't been updated for stateless trainer above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRNaiveBayesClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRNaiveBayesClassifier.py\n",
    "#HW 1.3\n",
    " \n",
    "from mrjob.job import MRJob\n",
    "import sys, re, string, operator, math, os\n",
    "\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "class MRNaiveBayesClassifier(MRJob):\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRNaiveBayesClassifier, self).__init__(*args, **kwargs)\n",
    "        self.zeroProb = 0\n",
    "\n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MRNaiveBayesClassifier, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf    \n",
    "\n",
    "    #load model from file; it has been sent from the master node to each worker node\n",
    "    def mapper_init(self):\n",
    "        self.modelStats = {}\n",
    "        \n",
    "        recordStrs = [s.split('\\n')[0].split('\\t') for s in open(\"StatelessModel1.txt\").readlines()]\n",
    "        for word, statsStr in recordStrs:\n",
    "            self.modelStats[word] = map(float, statsStr.split(\",\"))\n",
    "        \n",
    "        self.prC0 = math.log(self.modelStats[\"TomsPriors\"][2])\n",
    "        self.prC1 = math.log(self.modelStats[\"TomsPriors\"][3])\n",
    "        \n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        docID, docClass,text = line.split(\"\\t\",2)\n",
    "        text = text.strip()\n",
    "        text = regex.sub(' ', text.lower())\n",
    "        text = re.sub( '\\s+', ' ', text )\n",
    "        words = text.split()\n",
    "        \n",
    "        for word in words:\n",
    "            p0 = self.modelStats[word][2]\n",
    "            if self.modelStats[word][2] == 0.0:\n",
    "                self.zeroProb += 1\n",
    "            p1 = self.modelStats[word][3]\n",
    "            if self.modelStats[word][3] == 0.0:\n",
    "                self.zeroProb += 1\n",
    "            wordGivenHam = math.log(p0) if p0>0.0 else math.log(1)\n",
    "            wordGivenSpam = math.log(p1) if p1>0.0 else math.log(1)\n",
    "            prHAMGivenDoc = self.prC0 + wordGivenHam\n",
    "            prSPAMGivenDoc = self.prC1 + wordGivenSpam\n",
    "        \n",
    "        predictedClass = 1 #SPAM\n",
    "        if(prHAMGivenDoc > prSPAMGivenDoc):\n",
    "            predictedClass = 0 #HAM\n",
    "        if int(docClass) == predictedClass:\n",
    "            yield (docID, 0)  #no error\n",
    "        else: \n",
    "            yield (docID, 1) # error    \n",
    "        yield(\"zero\", self.zeroProb)\n",
    "    \n",
    "    def combiner(self, word, values):\n",
    "        for value in values:\n",
    "            yield (\"t\", value)\n",
    "            \n",
    "    def reducer(self, word, values):\n",
    "        zero = 0\n",
    "        numberOfRecords = 0\n",
    "        numberWrong = 0\n",
    "        for value in values:\n",
    "            if value > 1:\n",
    "                zero = value\n",
    "            else:    \n",
    "                numberOfRecords += 1\n",
    "                numberWrong += value\n",
    "        #print (numberOfRecords, numberWrong)\n",
    "        print ('Error rate: %.4f' %(1.0*numberWrong/float(numberOfRecords)))\n",
    "        print ('Number Wrong %d, Total Records %d'  %(numberWrong, numberOfRecords))\n",
    "        print ('number of word|class with 0 probability: %d' %(zero))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRNaiveBayesClassifier.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory c:\\users\\z030757\\appdata\\local\\temp\\MRNaiveBayesClassifier.Z030757.20160712.183624.513000\n",
      "Running step 1 of 1...\n",
      "Traceback (most recent call last):\n",
      "  File \"MRNaiveBayesClassifier.py\", line 92, in <module>\n",
      "    MRNaiveBayesClassifier.run()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\job.py\", line 430, in run\n",
      "    mr_job.execute()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\job.py\", line 448, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\launch.py\", line 160, in execute\n",
      "    self.run_job()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\launch.py\", line 230, in run_job\n",
      "    runner.run()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\runner.py\", line 473, in run\n",
      "    self._run()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\sim.py\", line 172, in _run\n",
      "    self._invoke_step(step_num, 'mapper')\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\sim.py\", line 259, in _invoke_step\n",
      "    working_dir, env)\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\inline.py\", line 157, in _run_step\n",
      "    child_instance.execute()\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\job.py\", line 439, in execute\n",
      "    self.run_mapper(self.options.step_num)\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\mrjob\\job.py\", line 504, in run_mapper\n",
      "    for out_key, out_value in mapper(key, value) or ():\n",
      "  File \"MRNaiveBayesClassifier.py\", line 50, in mapper\n",
      "    p0 = self.modelStats[word][2]\n",
      "KeyError: 'chinese'\n"
     ]
    }
   ],
   "source": [
    "!python MRNaiveBayesClassifier.py --jobconf mapred.reduce.tasks=1 chineseExample.txt --file=StatelessModel1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2\n",
      "3 x - 12\n",
      "12\n",
      "y = -9 * -1 + 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cube = 1\n",
    "square = 0\n",
    "n1 = -12\n",
    "c = 1\n",
    "\n",
    "x = -1\n",
    "y = (x**3)-(12*x)+1\n",
    "\n",
    "p = np.poly1d([cube,square,n1,c])\n",
    "p2 = np.polyder(p)\n",
    "print p2\n",
    "\n",
    "m = p2(x)\n",
    "\n",
    "b = -m*x+y\n",
    "\n",
    "\n",
    "tanline = m*x + b\n",
    "print tanline\n",
    "\n",
    "\n",
    "print (\"y = \"+str(m)+\" * \"+str(x)+\" + \"+str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cube = 1.0\n",
    "square = 0.0\n",
    "n1 = 2.0\n",
    "c = -4.0\n",
    "x = 1.0\n",
    "y = x**3+2*x-4\n",
    "p = np.poly1d([cube,square,n1,c])\n",
    "p2 = np.polyder(p)\n",
    "\n",
    "yprime = p2(x)\n",
    "\n",
    "if y>0.001:\n",
    "    nr = -1.0*(y/yprime)+x\n",
    "    x = nr\n",
    "else:\n",
    "    print nr\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.2\n",
      "2 1.17714285714\n",
      "3 1.17978019902\n",
      "4 1.17947792181\n",
      "5 1.17951259166\n",
      "6 1.17950861551\n",
      "7 1.17950907152\n",
      "8 1.17950901922\n",
      "9 1.17950902522\n",
      "10 1.17950902453\n",
      "11 1.17950902461\n",
      "12 1.1795090246\n",
      "13 1.1795090246\n",
      "14 1.1795090246\n",
      "15 1.1795090246\n",
      "16 1.1795090246\n",
      "17 1.1795090246\n"
     ]
    }
   ],
   "source": [
    "#basic Newton-Raphson in python:\n",
    "import numpy as np\n",
    "from sympy import *\n",
    "def f(x):\n",
    "    return x**3 + 2*x - 4\n",
    "def f2(x):\n",
    "    return 3*x + 2\n",
    "\n",
    "def nr_meth(a):\n",
    "    nr = a\n",
    "    i = 0\n",
    "    cnt = 0\n",
    "    while i != nr:\n",
    "        i = nr\n",
    "        cnt += 1\n",
    "        n = f(nr)*1.0\n",
    "        d = f2(nr)*1.0\n",
    "        nr = -1.0* (n/d) + nr\n",
    "        print cnt, nr\n",
    "\n",
    "nr_meth(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "x = Symbol('x')\n",
    "y = x**2 + 1\n",
    "yprime = y.diff(x)\n",
    "yprime\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
